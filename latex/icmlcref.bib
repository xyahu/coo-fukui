
@article{wang_narrowing_2024,
	title = {Narrowing the semantic gaps in {U}-{Net} with learnable skip connections: {The} case of medical image segmentation},
	volume = {178},
	issn = {08936080},
	shorttitle = {Narrowing the semantic gaps in {U}-{Net} with learnable skip connections},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608024004702},
	doi = {10.1016/j.neunet.2024.106546},
	language = {en},
	urldate = {2024-09-01},
	journal = {Neural Networks},
	author = {Wang, Haonan and Cao, Peng and Yang, Jinzhu and Zaiane, Osmar},
	month = oct,
	year = {2024},
	pages = {106546},
}

@inproceedings{guo_sa-unet_2021,
	address = {Milan, Italy},
	title = {{SA}-{UNet}: {Spatial} {Attention} {U}-{Net} for {Retinal} {Vessel} {Segmentation}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-8808-9},
	shorttitle = {{SA}-{UNet}},
	url = {https://ieeexplore.ieee.org/document/9413346/},
	doi = {10.1109/ICPR48806.2021.9413346},
	urldate = {2024-09-01},
	booktitle = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Guo, Changlu and Szemenyei, Marton and Yi, Yugen and Wang, Wenle and Chen, Buer and Fan, Changqi},
	month = jan,
	year = {2021},
	pages = {1236--1242},
	file = {已提交版本:C\:\\Zotero_files\\storage\\F9SXTHJF\\Guo 等 - 2021 - SA-UNet Spatial Attention U-Net for Retinal Vessel Segmentation.pdf:application/pdf},
}

@incollection{lian_u-net_2021,
	address = {Cham},
	title = {U-{Net} {Transformer}: {Self} and {Cross} {Attention} for {Medical} {Image} {Segmentation}},
	volume = {12966},
	isbn = {978-3-030-87588-6 978-3-030-87589-3},
	shorttitle = {U-{Net} {Transformer}},
	url = {https://link.springer.com/10.1007/978-3-030-87589-3_28},
	language = {en},
	urldate = {2024-09-01},
	booktitle = {Machine {Learning} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Petit, Olivier and Thome, Nicolas and Rambour, Clement and Themyr, Loic and Collins, Toby and Soler, Luc},
	editor = {Lian, Chunfeng and Cao, Xiaohuan and Rekik, Islem and Xu, Xuanang and Yan, Pingkun},
	year = {2021},
	doi = {10.1007/978-3-030-87589-3_28},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {267--276},
	file = {已提交版本:C\:\\Zotero_files\\storage\\JN5RGI7Q\\Petit 等 - 2021 - U-Net Transformer Self and Cross Attention for Medical Image Segmentation.pdf:application/pdf},
}

@inproceedings{pons_upsampling_2021,
	address = {Toronto, ON, Canada},
	title = {Upsampling {Artifacts} in {Neural} {Audio} {Synthesis}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-7605-5},
	url = {https://ieeexplore.ieee.org/document/9414913/},
	doi = {10.1109/ICASSP39728.2021.9414913},
	urldate = {2024-08-28},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Pons, Jordi and Pascual, Santiago and Cengarle, Giulio and Serra, Joan},
	month = jun,
	year = {2021},
	pages = {3005--3009},
	file = {已提交版本:C\:\\Zotero_files\\storage\\D69DGK4F\\Pons 等 - 2021 - Upsampling Artifacts in Neural Audio Synthesis.pdf:application/pdf},
}

@article{rodriguez-martinez_replacing_2022,
	title = {Replacing pooling functions in {Convolutional} {Neural} {Networks} by linear combinations of increasing functions},
	volume = {152},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608022001605},
	doi = {10.1016/j.neunet.2022.04.028},
	language = {en},
	urldate = {2024-08-28},
	journal = {Neural Networks},
	author = {Rodriguez-Martinez, Iosu and Lafuente, Julio and Santiago, Regivan H.N. and Dimuro, Graçaliz Pereira and Herrera, Francisco and Bustince, Humberto},
	month = aug,
	year = {2022},
	pages = {380--393},
	file = {全文:C\:\\Zotero_files\\storage\\TNLIY227\\Rodriguez-Martinez 等 - 2022 - Replacing pooling functions in Convolutional Neural Networks by linear combinations of increasing fu.pdf:application/pdf},
}

@inproceedings{murray_generalized_2014,
	address = {Columbus, OH, USA},
	title = {Generalized {Max} {Pooling}},
	isbn = {978-1-4799-5118-5},
	url = {https://ieeexplore.ieee.org/document/6909713},
	doi = {10.1109/CVPR.2014.317},
	abstract = {State-of-the-art patch-based image representations involve a pooling operation that aggregates statistics computed from local descriptors. Standard pooling operations include sum- and max-pooling. Sum-pooling lacks discriminability because the resulting representation is strongly inﬂuenced by frequent yet often uninformative descriptors, but only weakly inﬂuenced by rare yet potentially highlyinformative ones. Max-pooling equalizes the inﬂuence of frequent and rare descriptors but is only applicable to representations that rely on count statistics, such as the bag-ofvisual-words (BOV) and its soft- and sparse-coding extensions. We propose a novel pooling mechanism that achieves the same effect as max-pooling but is applicable beyond the BOV and especially to the state-of-the-art Fisher Vector – hence the name Generalized Max Pooling (GMP). It involves equalizing the similarity between each patch and the pooled representation, which is shown to be equivalent to re-weighting the per-patch statistics. We show on ﬁve public image classiﬁcation benchmarks that the proposed GMP can lead to signiﬁcant performance gains with respect to heuristic alternatives.},
	language = {en},
	urldate = {2024-08-28},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Murray, Naila and Perronnin, Florent},
	month = jun,
	year = {2014},
	pages = {2473--2480},
	file = {PDF:C\:\\Zotero_files\\storage\\EKJWC2DU\\Murray和Perronnin - 2014 - Generalized Max Pooling.pdf:application/pdf},
}

@article{zhao_improved_2024,
	title = {A improved pooling method for convolutional neural networks},
	volume = {14},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-51258-6},
	doi = {10.1038/s41598-024-51258-6},
	abstract = {Abstract
            The pooling layer in convolutional neural networks plays a crucial role in reducing spatial dimensions, and improving computational efficiency. However, standard pooling operations such as max pooling or average pooling are not suitable for all applications and data types. Therefore, developing custom pooling layers that can adaptively learn and extract relevant features from specific datasets is of great significance. In this paper, we propose a novel approach to design and implement customizable pooling layers to enhance feature extraction capabilities in CNNs. The proposed T-Max-Avg pooling layer incorporates a threshold parameter T, which selects the K highest interacting pixels as specified, allowing it to control whether the output features of the input data are based on the maximum values or weighted averages. By learning the optimal pooling strategy during training, our custom pooling layer can effectively capture and represent discriminative information in the input data, thereby improving classification performance. Experimental results show that the proposed T-Max-Avg pooling layer achieves good performance on three different datasets. When compared to LeNet-5 model with average pooling, max pooling, and Avg-TopK methods, the T-Max-Avg pooling method achieves the highest accuracy on CIFAR-10, CIFAR-100, and MNIST datasets.},
	language = {en},
	number = {1},
	urldate = {2024-08-28},
	journal = {Scientific Reports},
	author = {Zhao, Lei and Zhang, Zhonglin},
	month = jan,
	year = {2024},
	pages = {1589},
	file = {全文:C\:\\Zotero_files\\storage\\6LEEMV4A\\Zhao和Zhang - 2024 - A improved pooling method for convolutional neural networks.pdf:application/pdf},
}

@article{yao_pixel-wise_2018,
	title = {Pixel-wise regression using {U}-{Net} and its application on pansharpening},
	volume = {312},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218307008},
	doi = {10.1016/j.neucom.2018.05.103},
	language = {en},
	urldate = {2024-08-28},
	journal = {Neurocomputing},
	author = {Yao, Wei and Zeng, Zhigang and Lian, Cheng and Tang, Huiming},
	month = oct,
	year = {2018},
	pages = {364--371},
}

@article{armanious_medgan_2020,
	title = {{MedGAN}: {Medical} image translation using {GANs}},
	volume = {79},
	issn = {08956111},
	shorttitle = {{MedGAN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0895611119300990},
	doi = {10.1016/j.compmedimag.2019.101684},
	language = {en},
	urldate = {2024-08-28},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Armanious, Karim and Jiang, Chenming and Fischer, Marc and Küstner, Thomas and Hepp, Tobias and Nikolaou, Konstantin and Gatidis, Sergios and Yang, Bin},
	month = jan,
	year = {2020},
	pages = {101684},
	file = {已提交版本:C\:\\Zotero_files\\storage\\2MVGMG2R\\Armanious 等 - 2020 - MedGAN Medical image translation using GANs.pdf:application/pdf},
}

@inproceedings{deng_learning_2021,
	address = {Virtual Event China},
	title = {Learning {Contextual} {Transformer} {Network} for {Image} {Inpainting}},
	isbn = {978-1-4503-8651-7},
	url = {https://dl.acm.org/doi/10.1145/3474085.3475426},
	doi = {10.1145/3474085.3475426},
	language = {en},
	urldate = {2024-08-28},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Deng, Ye and Hui, Siqi and Zhou, Sanping and Meng, Deyu and Wang, Jinjun},
	month = oct,
	year = {2021},
	pages = {2529--2538},
}

@article{du_medical_2020,
	title = {Medical {Image} {Segmentation} based on {U}-{Net}: {A} {Review}},
	volume = {64},
	issn = {1943-3522},
	shorttitle = {Medical {Image} {Segmentation} based on {U}-{Net}},
	url = {https://library.imaging.org/jist/articles/64/2/jist0710},
	doi = {10.2352/J.ImagingSci.Technol.2020.64.2.020508},
	number = {2},
	urldate = {2024-08-28},
	journal = {Journal of Imaging Science and Technology},
	author = {Du, Getao and Cao, Xu and Liang, Jimin and Chen, Xueli and Zhan, Yonghua},
	month = mar,
	year = {2020},
	pages = {020508--1--020508--12},
}

@incollection{navab_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	volume = {9351},
	isbn = {978-3-319-24573-7 978-3-319-24574-4},
	shorttitle = {U-{Net}},
	url = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
	language = {en},
	urldate = {2024-08-28},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	doi = {10.1007/978-3-319-24574-4_28},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {234--241},
	file = {全文:C\:\\Zotero_files\\storage\\44QDU9YA\\Ronneberger 等 - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf},
}

@inproceedings{isola_image--image_2017,
	address = {Honolulu, HI},
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100115/},
	doi = {10.1109/CVPR.2017.632},
	urldate = {2024-08-28},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = jul,
	year = {2017},
	pages = {5967--5976},
	file = {已提交版本:C\:\\Zotero_files\\storage\\MUCUX8KJ\\Isola 等 - 2017 - Image-to-Image Translation with Conditional Adversarial Networks.pdf:application/pdf},
}

@inproceedings{zhu_unpaired_2017,
	address = {Venice},
	title = {Unpaired {Image}-to-{Image} {Translation} {Using} {Cycle}-{Consistent} {Adversarial} {Networks}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237506/},
	doi = {10.1109/ICCV.2017.244},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transﬁguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	language = {en},
	urldate = {2024-08-28},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = oct,
	year = {2017},
	pages = {2242--2251},
	file = {PDF:C\:\\Zotero_files\\storage\\MHMND3JL\\Zhu 等 - 2017 - Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.pdf:application/pdf},
}

@article{liu_deep_2018,
	title = {A deep learning approach for {18F}-{FDG} {PET} attenuation correction},
	volume = {5},
	issn = {2197-7364},
	url = {https://ejnmmiphys.springeropen.com/articles/10.1186/s40658-018-0225-8},
	doi = {10.1186/s40658-018-0225-8},
	language = {en},
	number = {1},
	urldate = {2024-08-28},
	journal = {EJNMMI Physics},
	author = {Liu, Fang and Jang, Hyungseok and Kijowski, Richard and Zhao, Gengyan and Bradshaw, Tyler and McMillan, Alan B.},
	month = dec,
	year = {2018},
	pages = {24},
	file = {全文:C\:\\Zotero_files\\storage\\RG93XQJK\\Liu 等 - 2018 - A deep learning approach for 18F-FDG PET attenuation correction.pdf:application/pdf},
}

@article{dong_synthetic_2019,
	title = {Synthetic {CT} generation from non-attenuation corrected {PET} images for whole-body {PET} imaging},
	volume = {64},
	issn = {1361-6560},
	url = {https://iopscience.iop.org/article/10.1088/1361-6560/ab4eb7},
	doi = {10.1088/1361-6560/ab4eb7},
	number = {21},
	urldate = {2024-08-28},
	journal = {Physics in Medicine \& Biology},
	author = {Dong, Xue and Wang, Tonghe and Lei, Yang and Higgins, Kristin and Liu, Tian and Curran, Walter J and Mao, Hui and Nye, Jonathon A and Yang, Xiaofeng},
	month = nov,
	year = {2019},
	pages = {215016},
	file = {PubMed Central Full Text PDF:C\:\\Zotero_files\\storage\\PIZBI9XZ\\Dong 等 - 2019 - Synthetic CT generation from non-attenuation corrected PET images for whole-body PET imaging.pdf:application/pdf},
}

@article{singh_automated_2023,
	title = {Automated nonlinear registration of coronary {PET} to {CT} angiography using pseudo-{CT} generated from {PET} with generative adversarial networks},
	volume = {30},
	issn = {10713581},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1071358123001447},
	doi = {10.1007/s12350-022-03010-8},
	language = {en},
	number = {2},
	urldate = {2024-08-28},
	journal = {Journal of Nuclear Cardiology},
	author = {Singh, Ananya and Kwiecinski, Jacek and Cadet, Sebastien and Killekar, Aditya and Tzolos, Evangelos and Williams, Michelle C and Dweck, Marc R. and Newby, David E. and Dey, Damini and Slomka, Piotr J.},
	month = apr,
	year = {2023},
	pages = {604--615},
	file = {PubMed Central Full Text PDF:C\:\\Zotero_files\\storage\\UYU5P3F9\\Singh 等 - 2023 - Automated nonlinear registration of coronary PET to CT angiography using pseudo-CT generated from PE.pdf:application/pdf},
}

@inproceedings{zeng_swin-casunet_2022,
	address = {Montreal, QC, Canada},
	title = {Swin-{CasUNet}: {Cascaded} {U}-{Net} with {Swin} {Transformer} for {Masked} {Face} {Restoration}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-9062-7},
	shorttitle = {Swin-{CasUNet}},
	url = {https://ieeexplore.ieee.org/document/9956183/},
	doi = {10.1109/ICPR56361.2022.9956183},
	urldate = {2024-08-28},
	booktitle = {2022 26th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	publisher = {IEEE},
	author = {Zeng, Chengbin and Liu, Yi and Song, Chunli},
	month = aug,
	year = {2022},
	pages = {386--392},
}

@article{jiang_rmau-net_2023,
	title = {{RMAU}-{Net}: {Residual} {Multi}-{Scale} {Attention} {U}-{Net} {For} liver and tumor segmentation in {CT} images},
	volume = {158},
	issn = {00104825},
	shorttitle = {{RMAU}-{Net}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482523003037},
	doi = {10.1016/j.compbiomed.2023.106838},
	language = {en},
	urldate = {2025-05-26},
	journal = {Computers in Biology and Medicine},
	author = {Jiang, Linfeng and Ou, Jiajie and Liu, Ruihua and Zou, Yangyang and Xie, Ting and Xiao, Hanguang and Bai, Ting},
	month = may,
	year = {2023},
	pages = {106838},
}

@incollection{descoteaux_medical_2017,
	address = {Cham},
	title = {Medical {Image} {Synthesis} with {Context}-{Aware} {Generative} {Adversarial} {Networks}},
	volume = {10435},
	isbn = {978-3-319-66178-0 978-3-319-66179-7},
	url = {https://link.springer.com/10.1007/978-3-319-66179-7_48},
	language = {en},
	urldate = {2025-05-26},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} − {MICCAI} 2017},
	publisher = {Springer International Publishing},
	author = {Nie, Dong and Trullo, Roger and Lian, Jun and Petitjean, Caroline and Ruan, Su and Wang, Qian and Shen, Dinggang},
	editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
	year = {2017},
	doi = {10.1007/978-3-319-66179-7_48},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {417--425},
}

@article{balakrishnan_voxelmorph_2019,
	title = {{VoxelMorph}: {A} {Learning} {Framework} for {Deformable} {Medical} {Image} {Registration}},
	volume = {38},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0278-0062, 1558-254X},
	shorttitle = {{VoxelMorph}},
	url = {https://ieeexplore.ieee.org/document/8633930/},
	doi = {10.1109/TMI.2019.2897538},
	number = {8},
	urldate = {2025-05-26},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Balakrishnan, Guha and Zhao, Amy and Sabuncu, Mert R. and Guttag, John and Dalca, Adrian V.},
	month = aug,
	year = {2019},
	pages = {1788--1800},
}

@inproceedings{lehtinen_noise2noise_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{Noise2Noise}: {Learning} {Image} {Restoration} without {Clean} {Data}},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/lehtinen18a.html},
	abstract = {We apply basic statistical reasoning to signal reconstruction by machine learning - learning to map corrupted observations to clean signals - with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans - all corrupted by different processes - based on noisy data only.},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {2965--2974},
}

@incollection{tsaftaris_deep_2017,
	address = {Cham},
	title = {Deep {MR} to {CT} {Synthesis} {Using} {Unpaired} {Data}},
	volume = {10557},
	isbn = {978-3-319-68126-9 978-3-319-68127-6},
	url = {http://link.springer.com/10.1007/978-3-319-68127-6_2},
	urldate = {2025-05-26},
	booktitle = {Simulation and {Synthesis} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Wolterink, Jelmer M. and Dinkla, Anna M. and Savenije, Mark H. F. and Seevinck, Peter R. and Van Den Berg, Cornelis A. T. and Išgum, Ivana},
	editor = {Tsaftaris, Sotirios A. and Gooya, Ali and Frangi, Alejandro F. and Prince, Jerry L.},
	year = {2017},
	doi = {10.1007/978-3-319-68127-6_2},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {14--23},
}

@inproceedings{tan_efficientnet_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	volume = {97},
	url = {https://proceedings.mlr.press/v97/tan19a.html},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tan, Mingxing and Le, Quoc},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = jun,
	year = {2019},
	pages = {6105--6114},
}

@inproceedings{vaswani_attention_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Attention is all you need},
	isbn = {978-1-5108-6096-4},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	note = {event-place: Long Beach, California, USA},
	pages = {6000--6010},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	urldate = {2025-05-26},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
	file = {已接受版本:C\:\\Zotero_files\\storage\\L3YW8IYD\\He 等 - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@inproceedings{zhang_improved_2018,
	address = {Banff, AB, Canada},
	title = {Improved {Adam} {Optimizer} for {Deep} {Neural} {Networks}},
	isbn = {978-1-5386-2542-2},
	url = {https://ieeexplore.ieee.org/document/8624183/},
	doi = {10.1109/IWQoS.2018.8624183},
	urldate = {2025-04-17},
	booktitle = {2018 {IEEE}/{ACM} 26th {International} {Symposium} on {Quality} of {Service} ({IWQoS})},
	publisher = {IEEE},
	author = {Zhang, Zijun},
	month = jun,
	year = {2018},
	pages = {1--2},
}

@article{pan_loss_2020,
	title = {Loss {Functions} of {Generative} {Adversarial} {Networks} ({GANs}): {Opportunities} and {Challenges}},
	volume = {4},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2471-285X},
	shorttitle = {Loss {Functions} of {Generative} {Adversarial} {Networks} ({GANs})},
	url = {https://ieeexplore.ieee.org/document/9098081/},
	doi = {10.1109/TETCI.2020.2991774},
	number = {4},
	urldate = {2025-04-17},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	author = {Pan, Zhaoqing and Yu, Weijie and Wang, Bosi and Xie, Haoran and Sheng, Victor S. and Lei, Jianjun and Kwong, Sam},
	month = aug,
	year = {2020},
	pages = {500--522},
}

@article{wang_dsg-gandual-stage-generator-based_2024,
	title = {{DSG}-{GAN}:{A} dual-stage-generator-based {GAN} for cross-modality synthesis from {PET} to {CT}},
	volume = {172},
	issn = {00104825},
	shorttitle = {{DSG}-{GAN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010482524003809},
	doi = {10.1016/j.compbiomed.2024.108296},
	language = {en},
	urldate = {2025-04-17},
	journal = {Computers in Biology and Medicine},
	author = {Wang, Huabin and Wang, Xiangdong and Liu, Fei and Zhang, Grace and Zhang, Gong and Zhang, Qiang and Lang, Michael L.},
	month = apr,
	year = {2024},
	pages = {108296},
}

@misc{li_large-scale_2020,
	title = {A {Large}-{Scale} {CT} and {PET}/{CT} {Dataset} for {Lung} {Cancer} {Diagnosis}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://www.cancerimagingarchive.net/collection/lung-pet-ct-dx/},
	doi = {10.7937/TCIA.2020.NNC2-0461},
	abstract = {This dataset consists of CT and PET-CT DICOM images of lung cancer subjects with XML Annotation files that indicate tumor location with bounding boxes. The images were retrospectively acquired from patients with suspicion of lung cancer, and who underwent standard-of-care lung biopsy and PET/CT. Subjects were grouped according to a tissue histopathological diagnosis. Patients with Names/IDs containing the letter 'A' were diagnosed with Adenocarcinoma, 'B' with Small Cell Carcinoma, 'C' with Large Cell Carcinoma, and 'G' with Squamous Cell Carcinoma.},
	urldate = {2025-04-17},
	publisher = {The Cancer Imaging Archive},
	author = {Li, Ping and Wang, Shuo and Li, Tang and Lu, Jingfeng and HuangFu, Yunxin and Wang, Dongxue},
	year = {2020},
}

@article{chai_root_2014,
	title = {Root mean square error ({RMSE}) or mean absolute error ({MAE})? – {Arguments} against avoiding {RMSE} in the literature},
	volume = {7},
	copyright = {https://creativecommons.org/licenses/by/3.0/},
	issn = {1991-9603},
	shorttitle = {Root mean square error ({RMSE}) or mean absolute error ({MAE})?},
	url = {https://gmd.copernicus.org/articles/7/1247/2014/},
	doi = {10.5194/gmd-7-1247-2014},
	abstract = {Abstract. Both the root mean square error (RMSE) and the mean absolute error (MAE) are regularly employed in model evaluation studies. Willmott and Matsuura (2005) have suggested that the RMSE is not a good indicator of average model performance and might be a misleading indicator of average error, and thus the MAE would be a better metric for that purpose. While some concerns over using RMSE raised by Willmott and Matsuura (2005) and Willmott et al. (2009) are valid, the proposed avoidance of RMSE in favor of MAE is not the solution. Citing the aforementioned papers, many researchers chose MAE over RMSE to present their model evaluation statistics when presenting or adding the RMSE measures could be more beneficial. In this technical note, we demonstrate that the RMSE is not ambiguous in its meaning, contrary to what was claimed by Willmott et al. (2009). The RMSE is more appropriate to represent model performance than the MAE when the error distribution is expected to be Gaussian. In addition, we show that the RMSE satisfies the triangle inequality requirement for a distance metric, whereas Willmott et al. (2009) indicated that the sums-of-squares-based statistics do not satisfy this rule. In the end, we discussed some circumstances where using the RMSE will be more beneficial. However, we do not contend that the RMSE is superior over the MAE. Instead, a combination of metrics, including but certainly not limited to RMSEs and MAEs, are often required to assess model performance.},
	language = {en},
	number = {3},
	urldate = {2025-04-01},
	journal = {Geoscientific Model Development},
	author = {Chai, T. and Draxler, R. R.},
	month = jun,
	year = {2014},
	pages = {1247--1250},
	file = {全文:C\:\\Zotero_files\\storage\\VX4UXYC6\\Chai和Draxler - 2014 - Root mean square error (RMSE) or mean absolute error (MAE) – Arguments against avoiding RMSE in the.pdf:application/pdf},
}

@inproceedings{hore_image_2010,
	address = {Istanbul, Turkey},
	title = {Image {Quality} {Metrics}: {PSNR} vs. {SSIM}},
	isbn = {978-1-4244-7542-1},
	shorttitle = {Image {Quality} {Metrics}},
	url = {http://ieeexplore.ieee.org/document/5596999/},
	doi = {10.1109/ICPR.2010.579},
	urldate = {2025-04-01},
	booktitle = {2010 20th {International} {Conference} on {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Hore, Alain and Ziou, Djemel},
	month = aug,
	year = {2010},
	pages = {2366--2369},
}

@article{zhou_wang_image_2004,
	title = {Image quality assessment: from error visibility to structural similarity},
	volume = {13},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1057-7149, 1941-0042},
	shorttitle = {Image quality assessment},
	url = {https://ieeexplore.ieee.org/document/1284395/},
	doi = {10.1109/TIP.2003.819861},
	number = {4},
	urldate = {2025-04-01},
	journal = {IEEE Transactions on Image Processing},
	author = {{Zhou Wang} and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
	month = apr,
	year = {2004},
	pages = {600--612},
}

@inproceedings{deng_wfacenets_2020,
	address = {Xiamen China},
	title = {{WFaceNets}: {A} {Series} of {Efficient} {Neural} {Networks} for {Face} {Validation} {Tasks}},
	isbn = {978-1-4503-8783-5},
	shorttitle = {{WFaceNets}},
	url = {https://dl.acm.org/doi/10.1145/3436369.3436461},
	doi = {10.1145/3436369.3436461},
	language = {en},
	urldate = {2025-05-26},
	booktitle = {Proceedings of the 2020 9th {International} {Conference} on {Computing} and {Pattern} {Recognition}},
	publisher = {ACM},
	author = {Deng, Xiaoyu and Wang, Huabin and Qiao, Biao and Tao, Liang},
	month = oct,
	year = {2020},
	pages = {192--199},
}

@inproceedings{hu_squeeze-and-excitation_2018,
	address = {Salt Lake City, UT},
	title = {Squeeze-and-{Excitation} {Networks}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578843/},
	doi = {10.1109/CVPR.2018.00745},
	urldate = {2025-05-26},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Hu, Jie and Shen, Li and Sun, Gang},
	month = jun,
	year = {2018},
	pages = {7132--7141},
	file = {已提交版本:C\:\\Zotero_files\\storage\\FHAHYUTS\\Hu 等 - 2018 - Squeeze-and-Excitation Networks.pdf:application/pdf},
}

@inproceedings{howard_searching_2019,
	address = {Seoul, Korea (South)},
	title = {Searching for {MobileNetV3}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-4803-8},
	url = {https://ieeexplore.ieee.org/document/9008835/},
	doi = {10.1109/ICCV.2019.00140},
	urldate = {2025-05-26},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Howard, Andrew and Sandler, Mark and Chen, Bo and Wang, Weijun and Chen, Liang-Chieh and Tan, Mingxing and Chu, Grace and Vasudevan, Vijay and Zhu, Yukun and Pang, Ruoming and Adam, Hartwig and Le, Quoc},
	month = oct,
	year = {2019},
	pages = {1314--1324},
}
